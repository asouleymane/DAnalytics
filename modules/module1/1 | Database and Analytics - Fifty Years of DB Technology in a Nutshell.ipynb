{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1:  Fifty Years of Database Technology in a Nutshell\n",
    "## Topics Covered\n",
    "* The Relational model\n",
    "  * Data Warehousing and Decision Support\n",
    "  * Graph Databases\n",
    "  * Distributed Databases\n",
    "    * Resilient Distributed Data Sets\n",
    "    * PySpark\n",
    "    \n",
    "## Readings\n",
    " - [Relational Databases Concept Overview](./presentations/RelationalDatabases.pdf) This is a powerpoint slide deck of basic relational database concepts. Some of you may already be familiar with relational databases. You might also recall that we shared Python relational datbase code with you in the intro course, module three. \n",
    " - [Enterprise Information Management: An Overview of Data in the Enterprise](https://dm-academy.github.io/aitm/#_chapter_11_enterprise_information_management) Data science work often invovles interacting with Enterprise Data. There are many considerations you will encounter when you first work in an enterprise context on a data science project. This reading is a **bonus** that will give you the language you need to talk about database technology with enterprise professionals. (Author: Charles Betz)\n",
    "     - [**Data Warehousing and Analytics**](https://dm-academy.github.io/aitm/#_analytics_2) This is a section from the chapter above that focuses on data warehousing and analytics. (Author: Charles Betz)\n",
    " - [IT Infrastructure: What is That?](https://dm-academy.github.io/aitm/#_chapter_2_infrastructure_management) This section from the book above is a nice overvierw of the parts that are in \"IT Infrastructure\"; a concept you are likely to encounter in your organizations. (Author: Charles Betz)\n",
    " - [Introduction to Spark with Python](http://www.kdnuggets.com/2015/11/introduction-spark-python.html) This will feel heavy and techy. KDNuggets is like that. Do not worry about reading this for mastery right now. Instead, focus on getting a zoomed out sense of some of the things that Spark can do, and how it is useful. The short answer, which you will see in one of the videos, is that it allows the load of processing data to be distributed across a **cluster**. \n",
    " - [What is a Graph Database? Neo4j](https://neo4j.com/developer/graph-database/) Graph databases are a more recent innovation.  Though not yet in wide use, they hold promise for overcoming performance and complexity issues associated with linking data from diverse sources. \n",
    " \n",
    "## Reference Information\n",
    " - [PySpark References](http://spark.apache.org/docs/latest/api/python/)\n",
    " - [PySpark Worked Examples](https://vanishingcodes.wordpress.com/2016/06/09/pyspark-tutorial-building-a-random-forest-binary-classifier-on-unbalanced-dataset/)\n",
    " \n",
    "## Labs\n",
    "In these labs you will be able to notice the performance differences between a query executed on a straigh up database (Postgres) and a spark cluster, using the same data. \n",
    " - [Reviews Data on a Postgres Database](./labs/Reviews_Data_Postgres.ipynb)\n",
    " - [Reviews Data on a Spark Cluster](./labs/Reviews_Data_Spark.ipynb)\n",
    " \n",
    "## Practice Examples\n",
    " - [New York City Taxi Data](./practices/Module_1_Database-Taxi-Data.ipynb)\n",
    " - [The Nerd's Network Guide to Cocktails](./practices/Drinks.ipynb)\n",
    " \n",
    "## Exercises\n",
    " - [Choose Your Data Set](./exercises/exercise1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
