{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark Cluster Performance\n",
    "The purpose of this notebook is to demonstrate the advantages of a Spark cluster query as opposed to a Postgres database query. It is best to complete the entire notebook in one sitting, as opposed to leaving it and coming\n",
    "back later. If you choose to leave it and come back to it later, you most likely experience glitches in the server.\n",
    "It is also important to run each of these top to bottom, and WAIT until one cell is finished before starting the next.\n",
    "\n",
    "#### Database Structure\n",
    "* reviews\n",
    "\n",
    "|Field |\tType |\tAllow Null |\tDefault Value| \n",
    "|------|-------|-------|------|\n",
    "|product_id |\tvarchar(20) |\tYes\t| x |\n",
    "|price |\tfloat4\t|Yes | x  |\t\n",
    "|user_id |\tvarchar(50) |\tYes\t| x |\n",
    "|profile_name |\tvarchar(100) |\tYes\t | x |\n",
    "|time |\ttimestamp(6) WITH TIME ZONE |\tYes |x |\t\n",
    "|score |\tfloat4 |\tYes |x\t |\n",
    "|title |\ttext |\tYes | x |\t\n",
    "|summary |\ttext |\tYes\t  |  x|\n",
    "|text | \ttext |\tYes\t |x  |\n",
    "|helpfulness_score |\tfloat4 |\tYes  |x  |\t\n",
    "|helpfulness_count |\tfloat4 |\tYes\t |x |\n",
    "|counter_id\t | int4\t| No |\tnextval('reviews_counter_id_seq'::regclass) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Spark\n",
    "Creating the SQL context from the spark context, which is the entry point into the relational functionality of our spark cluster.  Importing libraries. We need the time library to use the time.time() function.\n",
    "\n",
    "**In each fo the cells below, make sure you wait for the [\\*] to turn into a number before hitting the next cell. This code does take a time to come back. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the DataFrame from the SQLContext created above.  The DataFrame is based on the contents of our JSON file. \n",
    "\n",
    "This file contains the content from the **Amazon Product Reviews Dataset**. DataFrames allow us to manipulate and interact with structured data via a domain-specific language. Our domain-specific language is Python. \n",
    "\n",
    "The code below creates the DataFrame from the SQLContext based on the contents of our JSON file. This file contains the content from the reviews database. DataFrames allow us to manipulate and interact with structured data via a domain-specific language. Our domain-specific language is Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EXCEPT, note that this cell will not come back and often leaves the [*] in the execution notation to the left. \n",
    "## The \"if\" block prevents you from re-reading the data into the variable if you already have! Saves time. Saves lives. \n",
    "if df is None:\n",
    "   df = sqlContext.read.json(\"In/reviews.json\")\n",
    "else:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This prints the schema. \n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a basic query that selects twenty scores (a.k.a. ratings) of different movies and TV shows. The 'twenty'\n",
    "# is a constraint on the system to prevent the accidental retrieval of about thirty-four million rows. \n",
    "# This query does not specify how many to return.\n",
    "\n",
    "df2 = df.select(\"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can register a DataFrame as a table for the purpose of running an SQL query on the dataset. \n",
    "\n",
    "sqlContext.registerDataFrameAsTable(df, \"reviews\")\n",
    "df3 = sqlContext.sql(\"SELECT productId, price from reviews limit 5\")\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we will run a more time-consuming query since our dataset is gigantic. (When I ran this, it took almost \n",
    "# five minutes, and my internet connection is fairly good.)\n",
    "\n",
    "start = time.time()\n",
    "df4 = sqlContext.sql(\"SELECT COUNT(DISTINCT title) FROM reviews\")\n",
    "df4.collect()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You should see a large difference between this time and the runtime of the postgres query. (Mine was about half\n",
    "# the time of the postgres query.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Look at scores great than 4 \n",
    "start = time.time()\n",
    "df4 = df.filter(df.score>4).show()\n",
    "#df4.collect()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets look at helpfulness scores greater than 40\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "numeric_filtered = df.where(\n",
    "    (col('helpfuless_score')  > 40))\n",
    "numeric_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets look at helpfulness scores greater than 40 and a score greater than 4\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "numeric_filtered = df.where(\n",
    "    (col('helpfuless_score')  > 40))\n",
    "numeric_filtered.show()\n",
    "\n",
    "numeric_filtered2 = df.where(\n",
    "    (col('score')  > 4))\n",
    "numeric_filtered2.show()\n",
    "\n",
    "# I print both out so that you can see the first group has the helpfulness score greater than 40 and the second group \n",
    "# meets that condition AND has a score greater than 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagine Your Own Queries of the Amazon Reviews Database\n",
    "What are some queries you might imagine for Amazon product reviews?\n",
    " - Is there a product keyword you would like to know the average rating for?\n",
    " - What ratings levels are the most useful? High or low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Insert your queries here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
