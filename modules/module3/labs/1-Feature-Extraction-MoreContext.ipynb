{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Module 2 - Lab 1: Feature Extraction\n",
    "The pupose of this lab is to practice preparing data to be used with Spark. Here will learn how to obtain, process, and prepare the data. This is necessary because a dataset with an inconsistent format will be difficult to query and manage. Upon completing this lab, you should be able to load different datasets, query the datasets, transform the data (if needed), and extract features from the data that are useful for your study of the data. \n",
    "\n",
    "You can think of these preperation steps as a spark specific implmentation of some of the concepts of data cleaning that you learned in the Intro class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>231</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n",
      "done!"
     ]
    }
   ],
   "source": [
    "# you need to run some cell to get pyspark started.\n",
    "\n",
    "# Make Random String\n",
    "\n",
    "import os, random, string\n",
    "\n",
    "length = 32\n",
    "\n",
    "chars = string.ascii_letters + string.digits + '_-'\n",
    "\n",
    "random.seed = (os.urandom(1024))\n",
    "\n",
    "rndfolder = ''.join(random.choice(chars) for i in range(length))\n",
    "\n",
    "dirpath = '/home/hadoop/work_dir/' + rndfolder + '/'\n",
    "\n",
    "# Set Path and permissions (\"0770\", which means everyone can read and write the file) \n",
    "os.mkdir(dirpath, 0770)\n",
    "os.chdir(dirpath)\n",
    "\n",
    "print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will explain what has happened in the code block above. \n",
    "In the above code block, we import modules in order to access the methods we need. \n",
    "Without the modules, we can't use the methods that the modules possess.\n",
    "\n",
    "We assign the value 32 to the variable 'length'.\n",
    "This will be the length of our random folder name.\n",
    "\n",
    "We include all of the ascii letters, digits, and symbols '_' and '-' in the varible 'chars'.\n",
    "We will use this to select characters randomly for our random folder name.\n",
    "\n",
    "The 'random' class implements pseudo-random number generators for a multitude of uses.\n",
    "The 'seed()' method needs a value in order to initialize the random number generator.\n",
    "The 'os.urandom()' method returns a string of random bytes to be used to initialize a generator.\n",
    "Ours returns 1024 bytes.\n",
    "\n",
    "This piece of code `rndfolder = ''.join(random.choice(chars) for i in range(length))` uses the 'join()' method is used on the string ''.\n",
    "It returns a concatentation of the strings in the iterable parameter provided to the method. \n",
    "In the parameter: \n",
    "* `random.choice(chars)` returns a random element from the chars variable,\n",
    "* `for i in range(length)` is a for loop that iterates range(length) times (which is 32)\n",
    "* So the `random.choice(chars)` returns an element once, and then, again for every iteration of the for loop\n",
    "\n",
    "The variable 'dirpath' is the string for the name of our random folder.\n",
    "The folder is necessary for our machine to return images. This is very simplified, but it is a densely complex matter, so I will spare you the details.\n",
    "\n",
    "This line `os.mkdir(dirpath, 0770)` creates a directory with the name 'dirpath' and provides the permission given in the second parameter.\n",
    "This line `os.chdir(dirpath)` changes the current working directory to that of our new directory (folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to prepare a figure and return a URL to it. We are using Amazon's \"Elastic Map Reduce\" Infrastructure to run SPARK. This Infrastructure has some limits to, among them, we cannot generate graphics on it. What we can do is write visualizations to a file, then Provide the URL where you can click and see the results. \n",
    "\n",
    "The `process_figure` function created below will be used to print URL's for output throughout this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World"
     ]
    }
   ],
   "source": [
    "# This defintion defines a function (or method) called 'process_figure'.\n",
    "# The function takes in two parameters called 'fig' and 'name', respectively.\n",
    "def process_figure(fig, name):\n",
    "    # The following line saves the figure. 'name' is the file name of the figure. \n",
    "    fig.savefig(name)\n",
    "    # printing\n",
    "    print 'http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/' + rndfolder + '/' + name\n",
    "\n",
    "# Importing several modules below and printing 'Hello World' at the end.\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # non-graphical mode (pngs(?))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "from scipy import sparse as sp\n",
    "\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "\n",
    "print \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the user dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the dataset here, and observe the data sturcture by looking at the first line of data. The first() function is similar to the collect() function, except that it only returns the first element of the RDD to the driver. The take(k) function could have been used to obtain the first k elements of the RDD.\n",
    "\n",
    "There are five fields in this data set:\n",
    " - user id\n",
    " - age\n",
    " - gender\n",
    " - occupation\n",
    " - zip code\n",
    " \n",
    "Below, we are creating a new RDD from the text of the file \"ml-100k/u.user\" with the function textFile(), which transforms the text in a file into an RDD. We then, grab the first line of the dataset and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'1|24|M|technician|85711'"
     ]
    }
   ],
   "source": [
    "user_data = sc.textFile(\"ml-100k/u.user\")\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we do the same transformation as we did above. We then, take the first ten lines of the dataset and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1|24|M|technician|85711', u'2|53|F|other|94043', u'3|23|M|writer|32067', u'4|24|M|technician|43537', u'5|33|F|other|15213', u'6|42|M|executive|98101', u'7|57|M|administrator|91344', u'8|36|M|administrator|05201', u'9|29|M|student|01002', u'10|53|M|lawyer|90703']"
     ]
    }
   ],
   "source": [
    "user_data = sc.textFile(\"ml-100k/u.user\")\n",
    "user_data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section of code, we obtain the count for each of the fields within each line where that makes sense. Knowing the number of users, genders, occupations and zip codes makes logical sense. Knowing the number of distinct ages makes less less sense, and we will work with age data later. \n",
    "\n",
    "This descriptive summary will give us an idea of how many values for each field there are. We are exploring the data. \n",
    "\n",
    "To do this, we first transform the data by splitting each line around the `|` character. This gives us an RDD ( [Resilient Distributed Data Set](http://spark.apache.org/examples.html) ) with each record being a Python list containing the user ID, age, gender, occupation, and ZIP code fields. Next, we count the number of users, genders, occupations, and ZIP codes. Note: We are NOT cacheing the data here because the RDD is small in size. \n",
    "\n",
    "This is where variables referenced in the code below for accessing the data are instantiated (created)\n",
    "The map() function applies the given function (in the parameter list) to every element in the object that it is called upon.\n",
    "A lambda function is an anonymous function, which is utilized on one particular instance and then discarded.\n",
    "\n",
    "The line `user_fields = user_data.map(lambda line: line.split(\"|\"))` splits every element `line` in user_data by the given character.\n",
    "\n",
    "The line `num_users = user_fields.map(lambda fields: fields[0]).count()` counts all of the elements in the first set of the fields array.\n",
    "\n",
    "The line `num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()` counts the distinct elements that are within the third set of the fields array. \n",
    "(We begin arrays with index 0, so '2' indicates the third value: 0, 1, 2, ....)\n",
    "\n",
    "The line `num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()` counts the distinct elements in the fourth set of the fields array.\n",
    "\n",
    "The line `num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()` also counts the distinct elements. However, it is the fifth set of the fields array.\n",
    "\n",
    "We then print several values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 943, genders: 2, occupations: 21, ZIP codes: 795"
     ]
    }
   ],
   "source": [
    "user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
    "\n",
    "num_users = user_fields.map(lambda fields: fields[0]).count()\n",
    "\n",
    "num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()\n",
    "\n",
    "num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\n",
    "\n",
    "num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()\n",
    "\n",
    "print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather the values for the ages present within the dataset. \n",
    "\n",
    "We use the map function to cast every value at `x[1]` in the user_fields dataset as an `int`. \n",
    "We use `collect()` to return a list of all of the objects within the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = user_fields.map(lambda x: int(x[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the values in a histogram. (Hence the `hist()` function.) The `normed=True` statement means that we have specified that we want the histogram to be normalized meaning that each subset represents the percentage of the overall data that falls into each subset. We can see that the ages of the MovieLens users are slightly skewed towards younger viewers. \n",
    "\n",
    "`bins` is a parameter that tells us how many different lines to put into the histogram. We could change the number of bins and see how it affects the appearance of the graph. Go ahead and give that a try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/ages.png"
     ]
    }
   ],
   "source": [
    "plt.hist(ages, bins=20, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "# remember when we defined the process_figure function earlier?\n",
    "# This is where we print out a URL \n",
    "process_figure(fig, 'ages.png')\n",
    "plt.clf() # clear the plot after you're done, or it will interfere with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below selects the occupation information from the dataset and plots the information in a bar graph, which allows us to see the distribution of data. \n",
    "\n",
    "Here, we look at the relative frequencies of the various occupations of the users. \n",
    "\n",
    "We use the MapReduce approach to count the occurrences of each occupation in the dataset. [Hadoop MapReduce](http://www.datavard.com/blog/hadoop-in-a-nutshell-technical-overview/) is a technology upon which Spark sits. It is, in short, a way of distributing processing over a number of different machines in a cluster. \n",
    "The key difference between Hadoop MapReduce and Spark is that Spark performs the processing in-memory processing.\n",
    "\n",
    "NOTE: `reduceByKey` : Explanation\n",
    "When called on a dataset of (K, V) pairs, \n",
    "returns a dataset of (K, V) pairs where the values for each key are aggregated \n",
    "using the given reduce function func, which must be of type (V,V) => V. \n",
    "In the example below we are adding up the number of people in each occupation\n",
    "More simply, this code is counting up all the people in each occupation.\n",
    "\n",
    "`count_by_occupation` is the counting of the total people in an occupation\n",
    "`user_fields` is defined above \n",
    "`map()` is a python function\n",
    "`lambda` is a way of definining an anonymous python function \n",
    "`fields` is part of the lambda function\n",
    "for more on lambda functions, see this link: \n",
    "* https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/\n",
    "\n",
    "The line of code `count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()` uses the `map()` function to apply the lambda expression to every field within the \n",
    "`user_fields` set. These fields are the occupations. We end up with a dataset of (K, V) pairs, which will be used by the `reduceByKey()` function.\n",
    "The `reduceByKey()` function is called on the dataset of tuples.  \n",
    "The lambda inside the `reduceByKey()` function returns the sum of 1 and the value at `fields[3]` for each tuple in the dataset. \n",
    "The `collect()` function returns a list that contains all of the elements in this RDD.\n",
    "\n",
    "The `x_axis1` variable is assigned the list of occupations from the dataset. The for loop inside the parentheses means this: For every line `c` in the `count_by_occupation` variable, add the value at the first index to our created numpy array. (To learn more about the Numpy library, visit this link: http://cs231n.github.io/python-numpy-tutorial/.) A Numpy array is a grid of values (all of which are the same type) and is indexed by a tuple of nonnegative integers. Numpy arrays are high-performance, multidimensional arrays and as such, are useful for graphs, visualization, and representation for scientific measures in general. \n",
    "\n",
    "The `y_axis1` variable is assigned the number of individuals for each occupation in a similar fashion to the `x_axis1` variable.\n",
    "\n",
    "We then print the values of both arrays. We can see the occupation list and the number of users per occupation. This comes from the calculations above. We got our answer from the first line of code in the following code block. We were able to obtain the occupations from each line in the RDD via the `map()` function/lambda statement combination and count the number of users per occupation via the `reduceByKey()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'administrator' u'writer' u'retired' u'student' u'doctor'\n",
      " u'entertainment' u'marketing' u'executive' u'none' u'scientist'\n",
      " u'educator' u'lawyer' u'healthcare' u'technician' u'librarian'\n",
      " u'programmer' u'artist' u'salesman' u'other' u'homemaker' u'engineer']\n",
      "[ 79  45  14 196   7  18  26  32   9  31  95  12  16  27  51  66  28  12\n",
      " 105   7  67]"
     ]
    }
   ],
   "source": [
    "count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "x_axis1 = np.array([c[0] for c in count_by_occupation])\n",
    "y_axis1 = np.array([c[1] for c in count_by_occupation])\n",
    "\n",
    "# The print statements below will help you see that the reduceByKey function called in count_by_occupation\n",
    "# is then put into variables we will next graph\n",
    "print(x_axis1)\n",
    "print(y_axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'administrator', 79), (u'writer', 45), (u'retired', 14), (u'student', 196), (u'doctor', 7), (u'entertainment', 18), (u'marketing', 26), (u'executive', 32), (u'none', 9), (u'scientist', 31), (u'educator', 95), (u'lawyer', 12), (u'healthcare', 16), (u'technician', 27), (u'librarian', 51), (u'programmer', 66), (u'artist', 28), (u'salesman', 12), (u'other', 105), (u'homemaker', 7), (u'engineer', 67)]"
     ]
    }
   ],
   "source": [
    "# You can also see this if we simply print count_by_occupation\n",
    "print(count_by_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once we have collected the RDD of counts per occupation, we convert it into two arrays for the x axis (which is the occupation) and the y axis (which represents the counts). The bar() function plots a bar graph. We need to the sort the count data so that the chart is ordered from the lowest to the highest count. This is a visual step, but understandable.\n",
    "\n",
    "The collect() function doesn't return the RDD in any particular order. \n",
    "\n",
    "We sort the data by first creating two numpy arrays, and then using argsort() to select the elements from each array ordered by the count data in an ascending fashion. \n",
    "\n",
    "The image shows that the most prevalent occupations are student, other, educator, adminstrator, engineer, and programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/occupation.png"
     ]
    }
   ],
   "source": [
    "x_axis = x_axis1[np.argsort(y_axis1)]\n",
    "y_axis = y_axis1[np.argsort(y_axis1)]\n",
    "\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis, width, color='lightblue')\n",
    "plt.xticks(rotation=30)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'occupation.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block does the following:\n",
    "* In this line `x_axis = x_axis1[np.argsort(y_axis1)]` the `np.argsort(y_axis1)` function takes in array to sort (in our case this is the `y_axis1` array), and then, returns the same array, except now, it is sorted.\n",
    "* The same story occurs for `y_axis`.\n",
    "* We then, set up our figure, which is a bar graph. We should see the occupations listed on the x axis, and the number of users in each occupation listed on the y axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query below counts the values for each occupation, and displays them. There were two different approaches used to count the values. countByValue() counts the occurrences of each unique value in the RDD and returns it to the driver as a Python dict() method. Notice that the results are the same for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countByValue approach:\n",
      "{u'administrator': 79, u'retired': 14, u'lawyer': 12, u'healthcare': 16, u'marketing': 26, u'executive': 32, u'scientist': 31, u'student': 196, u'technician': 27, u'librarian': 51, u'programmer': 66, u'salesman': 12, u'homemaker': 7, u'engineer': 67, u'none': 9, u'doctor': 7, u'writer': 45, u'entertainment': 18, u'other': 105, u'educator': 95, u'artist': 28}\n",
      "\n",
      "Map-reduce approach:\n",
      "{u'administrator': 79, u'executive': 32, u'retired': 14, u'doctor': 7, u'entertainment': 18, u'marketing': 26, u'writer': 45, u'none': 9, u'healthcare': 16, u'scientist': 31, u'homemaker': 7, u'student': 196, u'educator': 95, u'technician': 27, u'librarian': 51, u'programmer': 66, u'artist': 28, u'salesman': 12, u'other': 105, u'lawyer': 12, u'engineer': 67}"
     ]
    }
   ],
   "source": [
    "count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\n",
    "print \"countByValue approach:\"\n",
    "print dict(count_by_occupation2)\n",
    "print \"\"\n",
    "print \"Map-reduce approach:\"\n",
    "print dict(count_by_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring the [MovieLens](https://movielens.org) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens is not just *any* dataset. It is the original recommender system dataset, and exists as an ongoing project and important artifact in introduction of recommendation systems that are now part of our every day life. The conference where these systems originated is Dr. Goggins \"home conference\" academically. I did my masters degree at Minnesota, taking classes from each of the MovieLens founders in the [GroupLens](https://en.wikipedia.org/wiki/GroupLens_Research) research group. This is some fun stuff!\n",
    "\n",
    "Here, we repeat the above process, except with a different dataset - one about movies. :) You can see the structure of the first line of data within the dataset.\n",
    "\n",
    "We are looking at the **u.item** file from the [MovieLens 100k Database](http://files.grouplens.org/datasets/movielens/ml-100k-README.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
      "Movies: 1682"
     ]
    }
   ],
   "source": [
    "movie_data = sc.textFile('ml-100k/u.item')\n",
    "print movie_data.first()\n",
    "num_movies = movie_data.count()\n",
    "print 'Movies: %d' % num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below handles errors in the parsing of the \"release date\" field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_year(x):\n",
    "    try:\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our utility function:\n",
    "\n",
    "    convert_year(x) \n",
    "\n",
    "to parse the years of the release using the map transformation and collect the results. Once we have assigned the year 1900 to any errors when parsing the data, we can filter the data using the filter() function. \n",
    "\n",
    "The structure of the movie data is:\n",
    " - Movie ID :: **NOTE** The movie ids are the ones used in the u.data data set.\n",
    " - Movie Name & Release Year\n",
    " - Release DATE\n",
    " - URL\n",
    " - Genre Information Fields (19)  The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once.  \n",
    "   - unknown\n",
    "   - Action\n",
    "   - Adventure \n",
    "   - Animation \n",
    "   - Children's \n",
    "   - Comedy \n",
    "   - Crime \n",
    "   - Documentary \n",
    "   - Drama \n",
    "   - Fantasy\n",
    "   - Film-Noir \n",
    "   - Horror \n",
    "   - Musical\n",
    "   - Mystery\n",
    "   - Romance \n",
    "   - Sci-Fi\n",
    "   - Thriller \n",
    "   - War \n",
    "   - Western"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\n",
    "years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n",
    "years_filtered = years.filter(lambda x: x!= 1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times real-world datasets require some in-depth approaches to parsing the data. This example displays why data exploration is so important since many of these issues in data integrity and quality are noticed during this phase.\n",
    "\n",
    "After filtering the bad data, we can transform the list of movie release years into \"movie ages\" by subtracting the current year. We use the countByValue() function to compute the counts for each movie age, and then plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/movie_ages.png"
     ]
    }
   ],
   "source": [
    "movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\n",
    "values = movie_ages.values()\n",
    "bins = movie_ages.keys()\n",
    "plt.hist(values, bins=bins, color='lightblue', normed=True)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'movie_ages.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at the information about this dataset, you will see it was released in 1998!! And if you look at the age information, you will notice that the age is based on the time of the rating. So, how does the graph look different if we measure age from today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/movie_ages_now.png"
     ]
    }
   ],
   "source": [
    "# We change the 1998 in the first line to 2016\n",
    "movie_ages_now = years_filtered.map(lambda yr: 2016-yr).countByValue()\n",
    "values1 = movie_ages_now.values()\n",
    "bins1 = movie_ages_now.keys()\n",
    "plt.hist(values1, bins=bins1, color='lightblue', normed=True, range=(min(bins1), max(bins1)))\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_size_inches(16, 10)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "process_figure(fig1, 'movie_ages_now.png')\n",
    "plt.clf()\n",
    "\n",
    "## BONUS: Figure out why the plot is WRONG!\n",
    "## DOUBLE BONUS : Fix it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the rating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 100,000 ratings and dissimilar to the previous two datasets is the fact that these records are split with a tab character (\"\\t\"). We will run some basic queries on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "Ratings : 100000"
     ]
    }
   ],
   "source": [
    "rating_data_raw = sc.textFile('ml-100k/u.data')\n",
    "print rating_data_raw.first()\n",
    "num_ratings = rating_data_raw.count()\n",
    "print 'Ratings : %d' % num_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are loading in the dataset with the `textFile()` method as we have done previously in this module. \n",
    "We then, print the first line from the dataset.\n",
    "Then, using the `count()` method, we count the number of lines in the dataset.\n",
    "Then, we print the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we perform some calculations to describe the data set. Notice the difference between what I calculate for the average, and the alternate calculation using the method from numpy. \n",
    "\n",
    "Why do you think there is a difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min rating: 1\n",
      "Max rating: 5\n",
      "Average rating: 3.00\n",
      "Alternate Average Rating: 3.53\n",
      "Median rating: 4\n",
      "Average # of ratings per user: 106.00\n",
      "Average # of ratings per movie: 59.00"
     ]
    }
   ],
   "source": [
    "rating_data = rating_data_raw.map(lambda line: line.split('\\t'))\n",
    "ratings = rating_data.map(lambda fields: int(fields[2]))\n",
    "max_rating = ratings.reduce(lambda x, y: max(x,y))\n",
    "min_rating = ratings.reduce(lambda x, y: min(x,y))\n",
    "mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings\n",
    "mean_rating2 = np.mean(ratings.collect())\n",
    "median_rating = np.median(ratings.collect())\n",
    "\n",
    "ratings_per_user = num_ratings / num_users\n",
    "ratings_per_movie = num_ratings / num_movies\n",
    "\n",
    "print \"Min rating: %d\" % min_rating\n",
    "print \"Max rating: %d\" % max_rating\n",
    "print \"Average rating: %2.2f\" % mean_rating\n",
    "print \"Alternate Average Rating: %2.2f\" % mean_rating2\n",
    "print \"Median rating: %d\" % median_rating\n",
    "print \"Average # of ratings per user: %2.2f\" % ratings_per_user\n",
    "print \"Average # of ratings per movie: %2.2f\" % ratings_per_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block does the following:\n",
    "* `rating_data`: We use the `map()` method to apply the lambda expression to every line in the dataset\n",
    "    * The lambda expression uses the `split()` method to retrieve a list of the fields in a single line of the dataset\n",
    "* `ratings`: Here, we again use `map()`, but this lambda casts the third field in a list from `rating_data` as an `int`. The list of `int`s is then returned as a list and stored in the `ratings` variable.\n",
    "* `max_rating`: Here we are using the `reduce()` method, which performs a computation on a list and returns the result. \n",
    "    * Here we obtain maximum value of the dataset (i.e. we look at each list in the dataset, and find the maximum value among the lists).\n",
    "* `min_rating`: Something similar happens here. Except now, it's the minimum.\n",
    "* `mean_rating`: Here, we sum all of the ratings in the dataset and divide by the number of ratings, which gives us the mean value (or average).\n",
    "* `mean_rating2`: Here, we use the handy `mean()` method from the numpy module that we imported in the beginning.\n",
    "    * Hint: The reason we get different answers has to do types.... I'll let you look into that.\n",
    "* `median_rating`: Here we use the `median()` method from the numpy module to obtain the median value in the dataset.\n",
    "* Then, we print all of the values out for observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides the `stats()` function for RDDs. This is a super handy function, so remember it for the future! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0)"
     ]
    }
   ],
   "source": [
    "ratings.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon observing the results, we can see that the average rating given by a user to a movie is around 3.5 and median rating is 4. This leads us to expect that the distribution of ratings will be skewed towards slightly higher ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/ratings_distribution.png"
     ]
    }
   ],
   "source": [
    "count_by_ratings = ratings.countByValue()\n",
    "x_axis = np.array(count_by_ratings.keys())\n",
    "y_axis = np.array([float(c) for c in count_by_ratings.values()])\n",
    "y_axis_normed = y_axis / y_axis.sum()\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis_normed, width, color='lightblue')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'ratings_distribution.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - After seeing some summary statistics, it is clear that the distribution of ratings is skewed towards average to high ratings. We can also observe the distribution of the number of ratings made by each user. \n",
    "\n",
    " - Recall that we previously computed the rating_data RDD used in the preceding code by splitting the ratings with the tab character. We will now use the rating_data variable again to compute the distribution of ratings per user. First we extract the key value (the user ID) and the rating value from rating_data RDD. \n",
    "\n",
    " - Then, we will group the ratings by user ID using the  groupByKey() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 272), (2, 62), (3, 54), (4, 24), (5, 175)]"
     ]
    }
   ],
   "source": [
    "user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).groupByKey()\n",
    "\n",
    "user_ratings_by_user = user_ratings_grouped.map(lambda (k,v): (k, len(v)))\n",
    "user_ratings_by_user.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block above, we did the following:\n",
    "* `user_ratings_grouped`: Here we cast the first and third values of each list in the `rating_data` dataset, and return the two values as a tuple for each list.\n",
    "    * Then, we apply the `groupByKey()` method to the list of tuples, to group the tuples into a single sequence. \n",
    "* `user_ratings_by_user`: Here we apply the lambda expression to each of the tuples that were obtained from the previous line of code. \n",
    "    * What we're doing here is returning the row number from the RDD, and the length of the `v` value in a tuple. \n",
    "* We then, take the first 5 tuples from the list and present them in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the histogram() of number of ratings per user. You should see that most of the users give fewer than 100 ratings. The distribution does show that many users give hundreds of ratings, surprisingly. Though not in the 1998 dataset, Dr. Goggins is one of the users who has provided hundreds of ratings; and Johnny Depp would not be happy at how Dr. Goggins rates his films. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/7iRcf7wFXmtkEPgNnqVlFd8Dr-P3L4Ok/ratings_per_user_distrib.png"
     ]
    }
   ],
   "source": [
    "user_ratings_by_user_local = user_ratings_by_user.map(lambda (k,v): v).collect()\n",
    "plt.hist(user_ratings_by_user_local, bins=200, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,10)\n",
    "process_figure(fig, 'ratings_per_user_distrib.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - At this point we have \n",
    "    - Explored the dataset and \n",
    "    - Graphed Descriptive Statistics\n",
    " - Next up:\n",
    "    - cleaning the data\n",
    "    - Preparing analysis for sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and transforming your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can extract useful features for any [machine learning algorithms](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/), we will first need to clean up the data. We may also need to transform it in different ways to extract any useful features. \n",
    "\n",
    "Data transformation and feature extraction are two steps that are closely linked. In general, real-world datasets contain bad data, missing data points, and outliers. To deal with such events, we utilize the following options:\n",
    "\n",
    "* Filter out or remove records with bad or missing values.\n",
    "* Fill in bad or missing data.\n",
    "* Apply robust techniques to outliers.\n",
    "* Apply transformations to potential outliers.\n",
    "\n",
    "Below is an example of filling in bad or missing data, which is extremely common in big data science. Here, we assign a value to the data point that is equal ot the median year of release. Then, we compute the mean and median year of release after grabbing all of the year of release data, except the bad data point. We then use the where() function to find the index of the bad value in years_pre_processed_array. Finally, we use this index to assing the median release year to the bad value. \n",
    "\n",
    "You should see that the median release year is quite higher because of the skewed distribution of the years. While it's not always very straightforward to decide on precisely which fill-in value to use for a given situation, in this case, median makes sense because it will counter the high concentration of data in skewed years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean year of release: 1989\n",
      "Median year of release: 1995\n",
      "Index of '1900' after reassigning median: []"
     ]
    }
   ],
   "source": [
    "years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).collect()\n",
    "years_pre_processed_array = np.array(years_pre_processed)\n",
    "\n",
    "mean_year = np.mean(years_pre_processed_array[years_pre_processed_array != 1900])\n",
    "median_year = np.median(years_pre_processed_array[years_pre_processed_array != 1900])\n",
    "index_bad_data = np.where(years_pre_processed_array == 1900)[0][0]\n",
    "years_pre_processed_array[index_bad_data] = median_year\n",
    "\n",
    "print 'Mean year of release: %d' % mean_year\n",
    "print 'Median year of release: %d' % median_year\n",
    "print \"Index of '1900' after reassigning median: %s\" % np.where(years_pre_processed_array == 1900)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, you should see some familiar methods used, and the all too familiar lambda expression. \n",
    "See if you can understand what each line of code is doing. \n",
    "You already know `map()`, and you've seen how lambdas work. \n",
    "You may need to look up a couple of the methods used, such as array() and where().\n",
    "In the `index_bad_data` line of code, the `[0][0]` piece on the end of the line is an index into the numpy array.\n",
    "See if you follow the rest. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting useful features from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract actual features from the data with which our machine learning model can be trained. \n",
    "\n",
    "Features refer to the variables that we use to [train our model](http://docs.aws.amazon.com/machine-learning/latest/dg/training-ml-models.html). Almost all machine learning models ultimately work on numerical data in the form of vectors, so we need to convert raw data into numbers.\n",
    "\n",
    "Here are some categories of features:\n",
    "* Numerical features: typically real or integer numbers\n",
    "* Categorical features: refer to variables that can take one from a set of possible states at any given time\n",
    "* Text features: derived from the text content in the data\n",
    "* Other features: most other types of features are ultimately represented numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features cannot be used as input in their raw form, as they are not numbers. They are members of a set of possible values that the variable can take. From the pervious example, user occupation is a categorical variable that can take a handful of values. \n",
    "\n",
    "Categroical variables are also known as nominal variables where there is no concept of order between the values of the variable. If there is such a concept of order between variables, we refer to them as ordinal variables. \n",
    "\n",
    "To transform categorical variables into a numerical representation, we can use a common appraoch known as 1-of-k encoding. This is necessary to represent nominal variables in a way that makes sense for a machine leanring task. \n",
    "\n",
    " - Assume there are k possible values that the variable can take. If we assign each possible value an index from the set of 1 to k, then we can represent a given state of the variable using a binary vector of length k. Here, all entries are zero, except the entry at the index that corresponds to the given state of the varialbe. \n",
    "\n",
    " - Here, we collect all the possible states of the occupation variable. We then, assign index values to each possible occupation in turn.\n",
    "\n",
    "Basically, we are creating a map of occupations to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding of 'doctor': 2\n",
      "Encoding of 'programmer': 14"
     ]
    }
   ],
   "source": [
    "all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\n",
    "all_occupations.sort()\n",
    "\n",
    "idx = 0\n",
    "all_occupations_dict = {}\n",
    "for o in all_occupations:\n",
    "    all_occupations_dict[o] = idx\n",
    "    idx += 1\n",
    "\n",
    "print \"Encoding of 'doctor': %d\" % all_occupations_dict['doctor']\n",
    "print \"Encoding of 'programmer': %d\" % all_occupations_dict['programmer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, the first two lines contain pieces that look familiar.\n",
    "The middle chunk creates a dictionary. \n",
    "A python dictionary is a commonly used data structure in the language.\n",
    "It is, in essence, a list of key value pairs.\n",
    "Keys are separated from their values via a colon, and items are separated by commas. \n",
    "The whole thing is enclosed in curly braces `{}`.\n",
    "The keys must be unique, but the values do not have to be.\n",
    "The keys must be of an immutable data type.\n",
    "\n",
    "`all_occupations_dict`: initially, an empty dictionary\n",
    "The `for` loop steps through all of the occupations in the `all_occupations` list and assigns an index. \n",
    "\n",
    "Two examples are printed at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then, encode the value of programmer. We start by creating  a numpy array of a length that is equal to the number of possible occupations (k) and filling it with zeros. Then, we use the zeros() function to create the array. Then, we extract the index of the word programmer and assign a value of 1 to the array value at the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'administrator': 0, u'executive': 6, u'retired': 15, u'student': 18, u'doctor': 2, u'marketing': 11, u'entertainment': 5, u'none': 12, u'scientist': 17, u'writer': 20, u'healthcare': 7, u'other': 13, u'lawyer': 9, u'educator': 3, u'technician': 19, u'librarian': 10, u'programmer': 14, u'artist': 1, u'salesman': 16, u'homemaker': 8, u'engineer': 4}"
     ]
    }
   ],
   "source": [
    "print(all_occupations_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary feature vector: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "Length of binary vector: 21"
     ]
    }
   ],
   "source": [
    "K = len(all_occupations_dict)\n",
    "binary_x = np.zeros(K)\n",
    "k_programmer = all_occupations_dict['programmer']\n",
    "binary_x[k_programmer] = 1\n",
    "print \"Binary feature vector: %s\" % binary_x\n",
    "print \"Length of binary vector: %d\" % K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to compute a derived feature from one or more available variables. We can compute the average rating given by each user to all the movies they rated. This feature is one that can provide a user-specific intercept in our model. By taking the raw rating data and creating a new feature, we can now learn a better model. Often the idea behind using these types of transformations is to summarize the numerical data in a way that may make it easier for a model to learn. \n",
    "\n",
    "To illustrate this concept, we will use the times of the ratings given by users to movies. These are within the Unix timestamps format. We will use the datetime() module to extract the date and time from the timestamp, and in turn, extract the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_datetime(ts):\n",
    "    return datetime.datetime.fromtimestamp(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will use the map() transformation to extract the timestamp field. This converts it to a Python int datatype. We will then apply the extract_datetime() function to each of the timestamps and extract the hour from the resulting datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 19, 7, 5, 5]"
     ]
    }
   ],
   "source": [
    "timestamps = rating_data.map(lambda fields: int(fields[3]))\n",
    "hour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\n",
    "hour_of_day.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have transformed the raw time data into a categorical feature that represents the hour of the day in which the rating was given. We now demonstrate assigning each hour-of-the-day value into a defined \"bucket\" that represents a time of day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_tod(hr):\n",
    "    times_of_day = {\n",
    "        'morning': range(7, 12),\n",
    "        'lunch': range(12, 14),\n",
    "        'afternoon': range(14, 18),\n",
    "        'evening': range(18, 23),\n",
    "        'night': range(0, 7) + [23]\n",
    "    }\n",
    "    for k, v in times_of_day.iteritems():\n",
    "        if hr in v:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function definition does this:\n",
    "* Firstly, it takes in an hour as a parameter.\n",
    "* Then, it gives a time range to each key (denoted by the red text).\n",
    "    * Notice that the first value in all of the `range()` calls is less than the second value.\n",
    "    * This is how we present ranges validly in python.\n",
    "* After the `times_of_day` dictionary has been created, we use a `for` loop to step through the all of the items in the `times_of_day` dictionary.\n",
    "* The `if` statement states that if `hr` is found in one of the range values, then return that value's key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `assign_tod()` function to the hour of each rating event contained in the `hour_of_day` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afternoon', 'evening', 'morning', 'night', 'night']"
     ]
    }
   ],
   "source": [
    "time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\n",
    "time_of_day.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text features can be considered forms of categorical and derived features. The goal is to turn raw text into a form that is more amenable to machine learning. The field of natural language processing is dedicated to processing, representing, and modeling textual content. A full treatment is beyond the scope of this module. However, here we will use the bag-of-words representation, which is a simple and standard approach for text-feature extraction.\n",
    "\n",
    "This approach treats a piece of text content as a set of the words, and possibly numbers. The process is as follows:\n",
    "* Tokenization: First, tokenization is applied to the text ot split it into a set of tokens (i.e. words, numbers, etc.)\n",
    "* Stop word removal: Next, we remove very common words such as \"the\", \"and\", and \"but\" (a.k.a. stop words)\n",
    "* Stemming: Next, you can include stemming, which refers to taking a term and reducing it to its base form or stem. (Ex: Plural terms become singular.) \n",
    "* Vectorization: The final step is turning the processed terms into a vector representation. The simplest form is a binary vector representation where we assign a value of one if a term exists in the text and zero if it does not. (This is identical to the categorical 1-of-k encoding we encountered earlier.)\n",
    "\n",
    "The following is an example of extracting textual features in the binary vector representation. We will use the movie titles that are available. \n",
    "\n",
    "We firstly, create a function that strips away the year of release for each movie. We use Python's regular expression module, re, to search for hte year between parentheses in the movie titles. If a match is found, we extract only the title up to the index of the first match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_title(raw):\n",
    "    # non-word numbers between parentheses\n",
    "    grps = re.search('\\((\\w+)\\)', raw)\n",
    "    if grps:\n",
    "        # take only the titel, strip remaining whitespace\n",
    "        return raw[:grps.start()].strip()\n",
    "    else:\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the raw movie titles from the movie_fields RDD. Then, we test out our extract_title() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story\n",
      "GoldenEye\n",
      "Four Rooms\n",
      "Get Shorty\n",
      "Copycat"
     ]
    }
   ],
   "source": [
    "raw_titles = movie_fields.map(lambda fields: fields[1])\n",
    "for raw_title in raw_titles.take(5):\n",
    "    print extract_title(raw_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then like to apply our function to the raw titles and apply a tokenization scheme to the extracted titles to convert them to terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Toy', u'Story'], [u'GoldenEye'], [u'Four', u'Rooms'], [u'Get', u'Shorty'], [u'Copycat']]"
     ]
    }
   ],
   "source": [
    "movie_titles = raw_titles.map(lambda m: extract_title(m)) # tokenize on whitespace\n",
    "title_terms = movie_titles.map(lambda t: t.split(' '))\n",
    "print title_terms.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use Spark's flatMap() function to expand the list of strings in each record of hte title_terms RDD into a new RDD of strings where each record is a term called all_terms. \n",
    "\n",
    "We then print the total number of unique terms and test out our term mapping on a few different terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 2645\n",
      "Index of term 'Dead': 723\n",
      "Index of term 'Rooms': 321"
     ]
    }
   ],
   "source": [
    "all_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n",
    "idx = 0\n",
    "all_terms_dict = {}\n",
    "for term in all_terms:\n",
    "    all_terms_dict[term] = idx\n",
    "    idx += 1\n",
    "\n",
    "print \"Total number of terms: %d\" % len(all_terms_dict)\n",
    "print \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\n",
    "print \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flatMap()` function is similar to `map()`, but it also flattens the results.\n",
    "(Search the web for more about flattening results, if you are curious!)\n",
    "\n",
    "Spark's `zipWithIndex()` function is more efficient at acquiring this result. The function takes an RDD of values and merges them together with an index to create a new RDD of key-value pairs. \n",
    "\n",
    "(The key is the term and the value is the index in the term directory.) \n",
    "\n",
    "The `collectAsMap()` function collects the key-value RDD to the driver as a Python dict method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of term 'Dead': 723\n",
      "Index of term 'Rooms': 321"
     ]
    }
   ],
   "source": [
    "all_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "print \"Index of term 'Dead': %d\" % all_terms_dict2['Dead']\n",
    "print \"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a function that converts a set of terms into a sparse vector representation. To do this:\n",
    " - we create an empty sparse matrix with one row and a number of columns equal to the total number of terms in our dictionary. \n",
    " - We then step through each term in the input list of terms and check whether this term is in our term dictionary. \n",
    "  - If it is, we assign a value of 1 to the vector at the index that corresponds to the term in our dictionary mapping. \n",
    "\n",
    "Once we have the function, we apply it to each record in our RDD of extracted terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(terms, term_dict):\n",
    "    num_terms = len(term_dict)\n",
    "    x = sp.csc_matrix((1, num_terms))\n",
    "    for t in terms:\n",
    "        if t in term_dict:\n",
    "            idx = term_dict[t]\n",
    "            x[0, idx] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 1 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 1 stored elements in Compressed Sparse Column format>]"
     ]
    }
   ],
   "source": [
    "all_terms_bcast = sc.broadcast(all_terms_dict)\n",
    "term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\n",
    "term_vectors.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each movie title has been transformed into a sparse vector. We can see that the titles in which we extracted two terms have two non-zero entries in the vector, titles where we extracted only one term have one non-zero entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the features have been extracted into the form of a vector, a common preprocessing step is to normalize the numerical data. The general idea is to transform each numerical feature in a way that scales it to a standard size. \n",
    "\n",
    "There are different kinds of normalization, which are listed below:\n",
    "* Normalize a feature: This is usually a transformation applied to an individual feature across the dataset.\n",
    "* Normalize a feature vector: This is usually a transformation applied to all features in a given row of the dataset such that the resulting feature vector has a normalized length. This ensures that each feature in the vector is scaled such that the vector has a norm of 1.\n",
    "\n",
    "Below, we use the norm() function to achieve the vector normalization by first computing the L2 norm of a random vector and then dividing each element in the vector by this norm to create our normalized vector. Why Normalize?\n",
    " - In simple terms, normalization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). \n",
    " - If you don't do this your models may be too complex and overfit or too simple and underfit, either way giving poor predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "2-Norm of x: 2.5908\n",
      "Normalized x:\n",
      "[ 0.19172213 -0.05336737  0.24999534  0.58786029 -0.09037871 -0.09037237\n",
      "  0.60954584  0.29621508 -0.1812081   0.20941776]\n",
      "2-Norm of normalized_x: 1.0000"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10)\n",
    "norm_x_2 = np.linalg.norm(x)\n",
    "normalized_x = x / norm_x_2\n",
    "\n",
    "print \"X:\\n%s\" % x\n",
    "print \"2-Norm of x: %2.4f\" %norm_x_2\n",
    "print \"Normalized x:\\n%s\" % normalized_x\n",
    "print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "2-Norm of x: 2.5908\n",
      "Normalized x:\n",
      "[ 0.19172213 -0.05336737  0.24999534  0.58786029 -0.09037871 -0.09037237\n",
      "  0.60954584  0.29621508 -0.1812081   0.20941776]\n",
      "2-Norm of normalized_x: 1.0000"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "vector = sc.parallelize([x])\n",
    "\n",
    "normalized_x_mllib = normalizer.transform(vector).first().toArray()\n",
    "\n",
    "print \"x:\\n%s\" % x\n",
    "print \"2-Norm of x: %2.4f\" %norm_x_2\n",
    "print \"Normalized x:\\n%s\" % normalized_x_mllib\n",
    "print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x_mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
