{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Module 2 - Practice 1: Feature Extraction\n",
    "The pupose of this lab is to practice preparing data to be used with Spark. Here will learn how to obtain, process, and prepare the data. This is necessary because a dataset with an inconsistent format will be difficult to query and manage. Upon completing this lab, you should be able to load different datasets, query the datasets, transform the data (if needed), and extract features from the data that are useful for your study of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you need to run some cell to get pyspark started.\n",
    "\n",
    "# Make Random String\n",
    "import os, random, string\n",
    "length = 32\n",
    "chars = string.ascii_letters + string.digits + '_-'\n",
    "random.seed = (os.urandom(1024))\n",
    "\n",
    "rndfolder = ''.join(random.choice(chars) for i in range(length))\n",
    "dirpath = '/home/hadoop/work_dir/' + rndfolder + '/'\n",
    "\n",
    "# Set Path\n",
    "os.mkdir(dirpath, 0770)\n",
    "os.chdir(dirpath)\n",
    "\n",
    "# function to prepare a figure and return a URL to it.\n",
    "def process_figure(fig, name):\n",
    "    fig.savefig(name)\n",
    "    print 'http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/' + rndfolder + '/' + name\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # non-graphical mode (pngs(?))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "from scipy import sparse as sp\n",
    "\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "\n",
    "print \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the user dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the dataset here, and observe the data sturcture by looking at the first line of data. The first() function is similar to the collect() function, except that it only returns the first element of the RDD to the driver. The take(k) function could have been used to obtain the first k elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_data = sc.textFile(\"ml-100k/u.user\")\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we obtain the count for each of the fields within each line. It will give us an idea of how many values for each field there are. Firstly, we transform the data by splitting each line around the \"|\" character. This gives us an RDD with each record being a Python list containing the user ID, age, gender, occupation, and ZIP code fields. Next, we count the number of users, genders, occupations, and ZIP codes. Note: We are NOT cacheing the data here because the RDD is small in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
    "num_users = user_fields.map(lambda fields: fields[0]).count()\n",
    "num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()\n",
    "num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\n",
    "num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()\n",
    "print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather the values for the ages present within the dataset, and then we plot the values in a histogram. (Hence the hist() function.) The \"normed=True\" statement means that we have specified that we want the histogram to be normalized meaning that each subset represents the percentage of the overall data that falls into each subset. We can see that the ages of the MovieLens users are slightly skewed towards younger viewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = user_fields.map(lambda x: int(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(ages, bins=20, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'ages.png')\n",
    "plt.clf() # clear the plot after you're done, or it will interfere with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below selects the occupation information from the dataset and plots the information in a bar graph, which allows us to see the distribution of data. Here, we look at the relative grequencies of the various occupations of the users. We use the MapReduce approach to count the occurrences of each occupation in the dataset. Then, once we have collected the RDD of counts per occupation, we convert it into two arrays for the xa axis (which is the occupation) and the y axis (which represents the counts). The bar() function plots a bar graph. We need to the sort the count data so that the chart is ordered from the lowest to the highest count. The collect() function doesn't return the RDD in any particular order. We do this by first creating two numpy arrays, and then using argsort() to select the elements from each array ordered by the count data in an ascending fashion. The image shows that the most prevalent occupations are student, other, educator, adminstrator, engineer, and programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "x_axis1 = np.array([c[0] for c in count_by_occupation])\n",
    "y_axis1 = np.array([c[1] for c in count_by_occupation])\n",
    "\n",
    "x_axis = x_axis1[np.argsort(y_axis1)]\n",
    "y_axis = y_axis1[np.argsort(y_axis1)]\n",
    "\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis, width, color='lightblue')\n",
    "plt.xticks(rotation=30)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'occupation.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query below counts the values for each occupation, and displays them. There were two different approaches used to count the values. countByValue() counts the occurrences of each unique value in the RDD and returns it to the driver as a Python dict() method. Notice that the results are the same for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\n",
    "print \"countByValue approach:\"\n",
    "print dict(count_by_occupation2)\n",
    "print \"\"\n",
    "print \"Map-reduce approach:\"\n",
    "print dict(count_by_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring the movie dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we repeat the above process, except with a different dataset - one about movies. :) You can see the structure of the first line of data within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_data = sc.textFile('ml-100k/u.item')\n",
    "print movie_data.first()\n",
    "num_movies = movie_data.count()\n",
    "print 'Movies: %d' % num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below handles errors in the parsing of the \"release date\" field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_year(x):\n",
    "    try:\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our utility function to parse the years of the release using the map transformation and collect the results. Once we have assigned the year 1900 to any errors when parsing the data, we can filter the data using the filter() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\n",
    "years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n",
    "years_filtered = years.filter(lambda x: x!= 1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times real-world datasets require some in-depth approaches to parsing the data. This example displays why data exploration is so important since many of these issues in data integrity and quality are noticed during this phase.\n",
    "\n",
    "After filtering the bad data, we can transform the list of movie release years into \"movie ages\" by subtracting the current year. We use the countByValue() function to compute the counts for each movie age, and then plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\n",
    "values = movie_ages.values()\n",
    "bins = movie_ages.keys()\n",
    "plt.hist(values, bins=bins, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'movie_ages.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the rating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 100,000 ratings and dissimilar to the previous two datasets is the fact that these records are split with a tab character (\"\\t\"). We will run some basic queries on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating_data_raw = sc.textFile('ml-100k/u.data')\n",
    "print rating_data_raw.first()\n",
    "num_ratings = rating_data_raw.count()\n",
    "print 'Ratings : %d' % num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating_data = rating_data_raw.map(lambda line: line.split('\\t'))\n",
    "ratings = rating_data.map(lambda fields: int(fields[2]))\n",
    "max_rating = ratings.reduce(lambda x, y: max(x,y))\n",
    "min_rating = ratings.reduce(lambda x, y: min(x,y))\n",
    "mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings\n",
    "median_rating = np.median(ratings.collect())\n",
    "\n",
    "ratings_per_user = num_ratings / num_users\n",
    "ratings_per_movie = num_ratings / num_movies\n",
    "\n",
    "print \"Min rating: %d\" % min_rating\n",
    "print \"Max rating: %d\" % max_rating\n",
    "print \"Average rating: %2.2f\" % mean_rating\n",
    "print \"Median rating: %d\" % median_rating\n",
    "print \"Average # of ratings per user: %2.2f\" % ratings_per_user\n",
    "print \"Average # of ratings per movie: %2.2f\" % ratings_per_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides the stats() function for RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon observing the results, we can see that the average rating given by a user to a movie is around 3.5 and median rating is 4. This leads us to expect that the distribution of ratings will be skewed towards slightly higher ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_by_ratings = ratings.countByValue()\n",
    "x_axis = np.array(count_by_ratings.keys())\n",
    "y_axis = np.array([float(c) for c in count_by_ratings.values()])\n",
    "y_axis_normed = y_axis / y_axis.sum()\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis_normed, width, color='lightblue')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "process_figure(fig, 'ratings_distribution.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing some summary statistics, it is clear that hte distribution of ratings is skewed towards average to high ratings. We can also observe the distribution of the number of ratings made by each user. Recall that we previously computed the rating_data RDD used in the preceding code by splitting the ratings with the tab character. We will now use the rating_data variable again. To compute the distribution of ratings per user, we firstly extract the key value (the user ID) and the rating value from rating_data RDD. Then, we will group the ratings by user ID using the  groupByKey() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).groupByKey()\n",
    "\n",
    "user_ratings_by_user = user_ratings_grouped.map(lambda (k,v): (k, len(v)))\n",
    "user_ratings_by_user.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the histogram() of number of ratings per user. You should see that most of the users give fewer than 100 ratings. The distribution does show that many users give hundreds of ratings, surprisingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ratings_by_user_local = user_ratings_by_user.map(lambda (k,v): v).collect()\n",
    "plt.hist(user_ratings_by_user_local, bins=200, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,10)\n",
    "process_figure(fig, 'ratings_per_user_distrib.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and transforming your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can extract useful features for any machine learning algorithms, we will first need to clean up the data. We may also need to transform it in different to extract any useful features. Data transformation and feature extraction are two steps that are closely linked. In general, real-world datasets contain bad data, missing data points, and outliers. To deal with such events, we utilize the following options:\n",
    "\n",
    "* Filter out or remove records with bad or missing values.\n",
    "* Fill in bad or missing data.\n",
    "* Apply robust techniques to outliers.\n",
    "* Apply transformations to potential outliers.\n",
    "\n",
    "Below is an example of filling in bad or missing data, which is extremely common in big data analytics. Here, we assign a value to the data point that is equal ot the median year of release. Then, we compute the mean and median year of release after grabbing all of the year of release data, except the bad data point. We then use the where() function to find the index of the bad value in years_pre_processed_array. Finally, we use this index to assing the median release year to the bad value. \n",
    "\n",
    "You should see that the median release year is quite higher because of the skewed distribution of the years. While it's not always very straightforward to decide on precisely which fill-in value to use for a given situation, in this case, it is definitely feasible to use the median due to this skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).collect()\n",
    "years_pre_processed_array = np.array(years_pre_processed)\n",
    "\n",
    "mean_year = np.mean(years_pre_processed_array[years_pre_processed_array != 1900])\n",
    "median_year = np.median(years_pre_processed_array[years_pre_processed_array != 1900])\n",
    "index_bad_data = np.where(years_pre_processed_array == 1900)[0][0]\n",
    "years_pre_processed_array[index_bad_data] = median_year\n",
    "\n",
    "print 'Mean year of release: %d' % mean_year\n",
    "print 'Median year of release: %d' % median_year\n",
    "print \"Index of '1900' after reassigning median: %s\" % np.where(years_pre_processed_array == 1900)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting useful features from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract actual features from the data with which our machine learning model can be trained. \n",
    "\n",
    "Features refer to the variables that we use to train our model. Almost all machine learning models ultimately work on numerical data in the form of vectors, so we need to convert raw data into numbers.\n",
    "\n",
    "Here are some categories of features:\n",
    "* Numerical features: typically real or integer numbers\n",
    "* Categorical features: refer to variables that can take one from a set of possible states at any given time\n",
    "* Text features: derived from the text content in the data\n",
    "* Other features: most other types of features are ultimately represented numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features cannot be used as input in their raw form, as they are not numbers. They are members of a seto f possible values that the variable can take. From the pervious example, user occupation is a categorical variable that can take a handful of values. \n",
    "\n",
    "Categroical variables are also known as nominal variables where there is no concept of order between the values of the variable. If there is such a concept of order between variables, we refer to them as ordinal variables. \n",
    "\n",
    "To transorm categorical variables into a numerical representation, we can use a common appraoch known as 1-of-k encoding. This is necessary to represent nominal variables in a way that makes sense for a machine leanring task. \n",
    "\n",
    "Assume there are k possible values that the variable can take. If we assign each possible value an index from the set of 1 to k, then we can represent a given state of the variable using a binary vector of length k. Here, all entries are zero, except the entry at the index that corresponds to the given state of the varialbe. \n",
    "\n",
    "Here, we collect all the possible states of the occupation variable. We then, assign index values to each possible occupation in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\n",
    "all_occupations.sort()\n",
    "\n",
    "idx = 0\n",
    "all_occupations_dict = {}\n",
    "for o in all_occupations:\n",
    "    all_occupations_dict[o] = idx\n",
    "    idx += 1\n",
    "\n",
    "print \"Encoding of 'doctor': %d\" % all_occupations_dict['doctor']\n",
    "print \"Encoding of 'programmer': %d\" % all_occupations_dict['programmer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then, encode the value of programmer. We start by creating  a numpy array of a length that is equal to the number of possible occupations (k) and filling it with zeros. Then, we use the zeros() function to create the array. Then, we extract the index of the word programmer and assign a value of 1 to the array value at the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = len(all_occupations_dict)\n",
    "binary_x = np.zeros(K)\n",
    "k_programmer = all_occupations_dict['programmer']\n",
    "binary_x[k_programmer] = 1\n",
    "print \"Binary feature vector: %s\" % binary_x\n",
    "print \"Length of binary vector: %d\" % K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to compute a derived feature from one or more available variables. We can compute the average rating given by each user to all the movies they rated. This feature is one that can provide a user-specific intercept in our model. By taking the raw rating data and creating a new feature, we can now learn a better model. Often the idea behind using these types of transformations is to summarize the numerical data in a way that may make it easier for a model to learn. \n",
    "\n",
    "To illustrate this concept, we will use the times of the ratings given by users to movies. These are within the Unix timestamps format. We will use the datetime() module to extract the date and time from the timestamp, and in turn, extract the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_datetime(ts):\n",
    "    return datetime.datetime.fromtimestamp(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will use the map() transformation to extract the timestamp field. This converts it to a Python int datatype. We will then apply the extract_datetime() function to each of the timestamps and extract the hour from the resulting datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps = rating_data.map(lambda fields: int(fields[3]))\n",
    "hour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\n",
    "hour_of_day.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have transformed the raw time data into a categorical feature that represents the hour of hte day in which the rating was given. We now demonstrate assigning each hour-of-the-day value into a defined \"bucket\" that represents a time of day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_tod(hr):\n",
    "    times_of_day = {\n",
    "        'morning': range(7, 12),\n",
    "        'lunch': range(12, 14),\n",
    "        'afternoon': range(14, 18),\n",
    "        'evening': range(18, 23),\n",
    "        'night': range(0, 7) + [23] # this book suggests range(23, 7) which has NEVER been valid python.\n",
    "    }\n",
    "    for k, v in times_of_day.iteritems():\n",
    "        if hr in v:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the assign_tod() function to the hour of each rating event contained in the hour_of_day RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\n",
    "time_of_day.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text features can be considered forms of categorical and derived features. The goal is to turn raw text into a form that is more amenable to machine learning. The field of natural language processing is dedicated to processing, representing, and modeling textual content. A full treatment is beyond the scope of this module. However, here we will use the bag-of-words representation, which is a simple and standard approach for text-feature extraction.\n",
    "\n",
    "This approach treats a piece of text content as a set of the words, and possibly numbers. The process is as follows:\n",
    "* Tokenization: First, tokenization is applied to the text ot split it into a set of tokens (i.e. words, numbers, etc.)\n",
    "* Stop word removal: Next, we remove very common words such as \"the\", \"and\", and \"but\" (a.k.a. stop words)\n",
    "* Stemming: Next, you can include stemming, which refers to taking a term and reducing it to its base form or stem. (Ex: Plural terms become singular.) \n",
    "* Vectorization: The final step is turning the processed terms into a vector representation. The simplest form is a binary vector representation where we assign a value of one if a term exists in the text and zero if it does not. (This is identical to the categorical 1-of-k encoding we encountered earlier.)\n",
    "\n",
    "The following is an example of extracting textual features in the binary vector representation. We will use the movie titles that are available. \n",
    "\n",
    "We firstly, create a function that strips away the year of release for each movie. We use Python's regular expression module, re, to search for hte year between parentheses in the movie titles. If a match is found, we extract only the title up to the index of the first match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_title(raw):\n",
    "    # non-word numbers between parentheses\n",
    "    grps = re.search('\\((\\w+)\\)', raw)\n",
    "    if grps:\n",
    "        # take only the titel, strip remaining whitespace\n",
    "        return raw[:grps.start()].strip()\n",
    "    else:\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the raw movie titles from the movie_fields RDD. Then, we test out our extract_title() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_titles = movie_fields.map(lambda fields: fields[1])\n",
    "for raw_title in raw_titles.take(5):\n",
    "    print extract_title(raw_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then like to apply our function to the raw titles and apply a tokenization scheme to the extracted titles to convert them to terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_titles = raw_titles.map(lambda m: extract_title(m)) # tokenize on whitespace\n",
    "title_terms = movie_titles.map(lambda t: t.split(' '))\n",
    "print title_terms.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use Spark's flatMap() function to expand the list of strings in eac hrecord of hte title_terms RDD into a new RDD of strings where each record is a term called all_terms. We then print the total number of unique terms and test out our term mapping on a few different terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n",
    "idx = 0\n",
    "all_terms_dict = {}\n",
    "for term in all_terms:\n",
    "    all_terms_dict[term] = idx\n",
    "    idx += 1\n",
    "\n",
    "print \"Total number of terms: %d\" % len(all_terms_dict)\n",
    "print \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\n",
    "print \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's zipWithIndex() function is more efficient at acquiring this result. The function takes an RDD of values nad merges them together with an index to create a new RDD of key-value pairs. (The key is the term and the value is the index in the term directory.) The collectAsMap() function collects the key-value RDD to the driver as a Python dict method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "print \"Index of term 'Dead': %d\" % all_terms_dict2['Dead']\n",
    "print \"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a function that converts a set of terms into a sparse vector representation. To do this, we create an empty sparse matrix with one row nad a number of columns equal to the total number of terms in our dictionary. We then step through each term in the input list of terms and check whether this term is in our term dictionary. If it is, we assign a value of 1 to the vector at the index that corresponds to the term in our dictionary mapping. \n",
    "\n",
    "Once we have the function, we apply it to eac hrecord in our RDD of extracted terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(terms, term_dict):\n",
    "    num_terms = len(term_dict)\n",
    "    x = sp.csc_matrix((1, num_terms))\n",
    "    for t in terms:\n",
    "        if t in term_dict:\n",
    "            idx = term_dict[t]\n",
    "            x[0, idx] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_terms_bcast = sc.broadcast(all_terms_dict)\n",
    "term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\n",
    "term_vectors.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each movie title has been transformed into a sparse vector. We can see that the titles in which we extracted two terms have two non-zero entries in the vector, titles where we extracted only one term have one non-zero entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the features have been extracted into the form of a vector, a common preprocessing step is the normalize the numerical data. The general idea is to transform each numerical feature in a way that scales it to a standard size. \n",
    "\n",
    "There are different kinds of normalization, which are listed below:\n",
    "* Normalize a feature: This is usually a transformation applied to an individual feature across the dataset.\n",
    "* Normalize a feature vector: This is usually a transformation applied to all features in a given row of the dataset such that the resulting feature vector has a normalized length. This ensures that each feature in the vector is scaled such that the vector has a norm of 1.\n",
    "\n",
    "Below, we use the norm() function to achieve the vector normalization by first computing the L2 norm of a random vector and then dividing each element in the vecotr by this norm to create our normalized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10)\n",
    "norm_x_2 = np.linalg.norm(x)\n",
    "normalized_x = x / norm_x_2\n",
    "\n",
    "print \"X:\\n%s\" % x\n",
    "print \"2-Norm of x: %2.4f\" %norm_x_2\n",
    "print \"Normalized x:\\n%s\" % normalized_x\n",
    "print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "vector = sc.parallelize([x])\n",
    "\n",
    "normalized_x_mllib = normalizer.transform(vector).first().toArray()\n",
    "\n",
    "print \"x:\\n%s\" % x\n",
    "print \"2-Norm of x: %2.4f\" %norm_x_2\n",
    "print \"Normalized x:\\n%s\" % normalized_x_mllib\n",
    "print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x_mllib)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
