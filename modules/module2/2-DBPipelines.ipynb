{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2:  Data Pipelines\n",
    "## Topics Covered\n",
    "* Feature Extraction\n",
    "* Recommendation Systems\n",
    "* Data Pipelines\n",
    "    \n",
    "### Readings\n",
    " - [Relational Databases Concept Overview](./presentations/RelationalDatabases.pdf) This is a powerpoint slide deck of basic relational database concepts. Some of you may already be familiar with relational databases. You might also recall that we shared Python relational datbase code with you in the intro course, module three. \n",
    " - [Enterprise Information Management: An Overview of Data in the Enterprise](https://dm-academy.github.io/aitm/#_chapter_11_enterprise_information_management) Data science work often invovles interacting with Enterprise Data. There are many considerations you will encounter when you first work in an enterprise context on a data science project. This reading is a **bonus** that will give you the language you need to talk about database technology with enterprise professionals. (Author: Charles Betz)\n",
    "     - [**Data Warehousing and Analytics**](https://dm-academy.github.io/aitm/#_analytics_2) This is a section from the chapter above that focuses on data warehousing and analytics. (Author: Charles Betz)\n",
    " - [IT Infrastructure: What is That?](https://dm-academy.github.io/aitm/#_chapter_2_infrastructure_management) This section from the book above is a nice overvierw of the parts that are in \"IT Infrastructure\"; a concept you are likely to encounter in your organizations. (Author: Charles Betz)\n",
    " - [Introduction to Spark with Python](http://www.kdnuggets.com/2015/11/introduction-spark-python.html) This will feel heavy and techy. KDNuggets is like that. Do not worry about reading this for mastery right now. Instead, focus on getting a zoomed out sense of some of the things that Spark can do, and how it is useful. The short answer, which you will see in one of the videos, is that it allows the load of processing data to be distributed across a **cluster**. \n",
    " - [What is a Graph Database? Neo4j](https://neo4j.com/developer/graph-database/) Graph databases are a more recent innovation.  Though not yet in wide use, they hold promise for overcoming performance and complexity issues associated with linking data from diverse sources. \n",
    " \n",
    "### Dig Deeper Readings\n",
    " - [What Do Emoji's Mean](./readings/ICWSM2016_emoji.pdf) Data pipelines are increasingly including emoji information, which is ripe for misinterpretation. This is a thought provoking article along those lines. \n",
    "\n",
    "### Reference Information\n",
    " - [PySpark References](http://spark.apache.org/docs/latest/api/python/)\n",
    " - [PySpark Worked Examples](https://vanishingcodes.wordpress.com/2016/06/09/pyspark-tutorial-building-a-random-forest-binary-classifier-on-unbalanced-dataset/)\n",
    " \n",
    "## Labs\n",
    "Labs are focused in this module on the development of data pipelines in Spark, using Spark based machine learning algorithms. \n",
    " - [Feature Extraction in Spark](./labs/1-Feature-Extraction.ipynb)\n",
    " - [Recommendation Engines (Collaborative Filtering) in Spark](./labs/2-Recommendation-Engines.ipynb)\n",
    " \n",
    "## Practice Examples\n",
    " - [Design a Database and Analytics Pipeline](./practices/1-designDB.ipynb)\n",
    " \n",
    "## Exercises (Complete Practice Activity)\n",
    " - [Complete Database and Analytics Pipeline Design](./practices/1-designDB.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
