{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Module 2 - Lab 2 - Building a Recommendation Engine in Spark\n",
    "\n",
    "In this lab, we will build a recommendation model in Spark. \n",
    "\n",
    "The idea behind recommendation engines is to predict what people might like and to uncover relationships between items to aid in the discovery process. They are similar to search engines. However, unlike search engines they try to present individuals with relevant content that they did not necessarily search for or that they might not have heard of. \n",
    "\n",
    "Usually, a recommendation engine tries to model the connections between users and some type of item. They are most effective in two general scenarios:\n",
    "* Large number of available options for users: When there is a very large number of available items, it becomes increasingly difficult for the user to find something they want. \n",
    "* A significant degree of personal taste involved: When personal taste plays a large role in selection, recommendation models, which often utilize a wisdom of the crowd approach, can be helpful in discovering items based on the behavior of others that have similar taste profiles. \n",
    "\n",
    "Amazon's shopping cart recommendations are a good example. So are Netflix movie recommendations. \n",
    "\n",
    "There are many types of recommendation models. The two that are most prevalent are: content-based filtering and collaborative filtering. Content-based filtering attemps to use the content or attributes of an item, together with some notion of similarity between two pieces of content to generate items similar to a given item. Collaborative filtering is a form of wisdom of the crowd approach where the set of preferences of users for items with which they have not yet interacted. The underlying idea is the notion of similarity. \n",
    "\n",
    "The code snipet below initializes our pyspark kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>155</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n",
      "Spark Initialized"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "import org.jblas.DoubleMatrix\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.mllib.evaluation.RankingMetrics\n",
    "\n",
    "println(\"Spark Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from the MovieLens 100k dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are using the MovieLens dataset. Remember that the dataset consisted of the \n",
    " - user ID, \n",
    " - movie ID, \n",
    " - rating, and \n",
    " - timestamp \n",
    " \n",
    "fields, all separated by the tab character. We don't need the time when the rating was given to train the model, so we will only extract the first three fields after inspecting the raw dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: String = 196\t242\t3\t881250949"
     ]
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"ml-100k/u.data\")\n",
    "rawData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Array[String] = Array(196, 242, 3)"
     ]
    }
   ],
   "source": [
    "val rawRatings = rawData.map(_.split(\"\\t\").take(3))\n",
    "rawRatings.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res4: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)"
     ]
    }
   ],
   "source": [
    "val ratings = rawRatings.map {case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble)}\n",
    "ratings.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a map, then, of the ratings people made on individual movies, extracted as \"features\". A **feature** is simply a way of expressing something that's useful to us. How people rated movies can then be used to predict how other people might like movies. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training the recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted these simple features from our raw data, we are ready to proceed with the model training. We need to provide the correctly-parsed input RDD we just creaetd as well as our chosen model parameters. \n",
    "\n",
    "Let's train the model! The inputs required for the model are as follows:\n",
    "* rank: This refers to the number of factors in the model. Generally, the greater the number of factors, the better, but this has a direct impact on memory usage, both for computation and to store models for serving, particularly for large number of users or items. \n",
    "* iterations: This refers to the number of iterations to run. While each iteration is guaranteed to decrease the reconstruction error of the ratings matrix, our model should converge to a reasonable solution after relatively few iterations. Therefore, we don't need to run for too many iterations in most cases.\n",
    "* lambda: This parameter controls the regularizaion of our model. Thus, lamba controls overfitting. The higher the value of lambda, the more the regularization is applied. What constitutes a sensible value is very dependent on the size, nature, nad sparsity of the underlying data, and as with almost all machine learning models, the regularization parameter is somthing that should be tuned using out-of-sample test data and cross-validation approaches.\n",
    "\n",
    "We are using a rank of 50, a lambda value of 0.01, and 10 iterations to train our model.\n",
    "\n",
    "This returns a MatrixFactorizationModel object, which contains the user and item factors in the form of an RDD pair. These are called userFeatures and productFeatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res5: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[209] at mapValues at ALS.scala:255"
     ]
    }
   ],
   "source": [
    "val model = ALS.train(ratings, 50, 10, 0.01)\n",
    "model.userFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force the computation using the count Spark action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: Long = 1682"
     ]
    }
   ],
   "source": [
    "model.userFeatures.count\n",
    "model.productFeatures.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model, we can use it to make preditions. These predictions typically take one of two forms: recommendations for a given user and related or similar items for a given item.\n",
    "\n",
    "** This is how collaborative filtering works : In effect, the recommendation model will search for users who rated some films like you did. Then it will find movies that you have not rated, but that these other users who rated some of the same films you did, like ** \n",
    "\n",
    "In our case, we want to generate recommended items (*movies*) for a given user. This will take the form of a top-K list, which means that the K items that our model predicts will have the highest probability of hte user liking them. We do this by predicting the scores for each item (*movie*) and ranking the list based on the score. The exact method to perform this computation depends on the model involved. For now, we are sheilding you from these internal details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the factor matrices computed by our model to compute the predicted scores (or ratings) for a user. We will focus on the explicit rating case using MovieLens data. However, the approach is the same when using the implicit model. \n",
    "\n",
    "The predict() method below is used to compute a predict a score for a given user and item (movie) combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictRating: Double = 3.8825736731923217"
     ]
    }
   ],
   "source": [
    "val predictRating = model.predict(789, 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the top-K recommended items for a user, MatrixFactorizationModel provides a convenience method called recommendProducts(). This takes in two arguments: user and num, where user is the user ID, and num is the number of items to recommend. It returns the top num items ranked in the order of the predicted score. \n",
    "\n",
    "** This is pretty cool. Its how product recommendations work on major websites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(789,182,5.9027642432595)\n",
      "Rating(789,317,5.731498054760541)\n",
      "Rating(789,156,5.687498683675839)\n",
      "Rating(789,192,5.598312888241889)\n",
      "Rating(789,56,5.5976131209991395)\n",
      "Rating(789,87,5.545339018005654)\n",
      "Rating(789,58,5.537538780943374)\n",
      "Rating(789,641,5.438618026363299)\n",
      "Rating(789,530,5.391429476559729)\n",
      "Rating(789,157,5.362021180629817)"
     ]
    }
   ],
   "source": [
    "val userId = 789\n",
    "val K = 10\n",
    "val topKRecs = model.recommendProducts(userId, K)\n",
    "\n",
    "println(topKRecs.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give these recommendations a sense check by taking a quick look at the titles of hte movies a user has rated and hte recommended movies. Firstly, we have the load the movie data. We will gather the data as a Map[Int, String] method mapping the movie ID to the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res36: String = Frighteners, The (1996)"
     ]
    }
   ],
   "source": [
    "val movies = sc.textFile(\"ml-100k/u.item\")\n",
    "val titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt, array(1))).collectAsMap()\n",
    "titles(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe the movie ratings for user 789. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33"
     ]
    }
   ],
   "source": [
    "val moviesForUser = ratings.keyBy(_.user).lookup(789)\n",
    "println(moviesForUser.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the 10 movies with the highest ratings by sorting the moviesForUser collection using the rating field of the Rating object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Godfather, The (1972),5.0)\n",
      "(Trainspotting (1996),5.0)\n",
      "(Dead Man Walking (1995),5.0)\n",
      "(Star Wars (1977),5.0)\n",
      "(Swingers (1996),5.0)\n",
      "(Leaving Las Vegas (1995),5.0)\n",
      "(Bound (1996),5.0)\n",
      "(Fargo (1996),5.0)\n",
      "(Last Supper, The (1995),5.0)\n",
      "(Private Parts (1997),4.0)"
     ]
    }
   ],
   "source": [
    "moviesForUser.sortBy(-_.rating).take(10).map(rating => (titles(rating.product), rating.rating)).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the top 10 recommendations for the user and see what the titles are using the same approach as the one we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GoodFellas (1990),5.9027642432595)\n",
      "(In the Name of the Father (1993),5.731498054760541)\n",
      "(Reservoir Dogs (1992),5.687498683675839)\n",
      "(Raging Bull (1980),5.598312888241889)\n",
      "(Pulp Fiction (1994),5.5976131209991395)\n",
      "(Searching for Bobby Fischer (1993),5.545339018005654)\n",
      "(Quiz Show (1994),5.537538780943374)\n",
      "(Paths of Glory (1957),5.438618026363299)\n",
      "(Man Who Would Be King, The (1975),5.391429476559729)\n",
      "(Platoon (1986),5.362021180629817)"
     ]
    }
   ],
   "source": [
    "topKRecs.map(rating => (titles(rating.product), rating.rating)).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating similar movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create our own code to generate similar movies. We will use the cosine similarity metric and the jblas linear algebra library to compute the required vector dot products. \n",
    "\n",
    "**WHAT?!** : We are looking for movies that are similar to each other.\n",
    "\n",
    "Then, we will compare the factor vector of our chosen item with each of the other items, using our similarity metric. Firstly, we create a vector object out of the factor vectors, which are in the form of an Array[Double]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aMatrix: org.jblas.DoubleMatrix = [1.000000; 2.000000; 3.000000]"
     ]
    }
   ],
   "source": [
    "val aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a method to compute the cosine similarity between two vectors. Cosine similarity is a measure of the angle between two vectors in an n-dimensional space. We compute this value by first calculating the dot product between the vectors and then dividing the result by a denominator, which is the norm (or length) of each vector multiplied together. In this way, cosine similarity is a normalized dot product. \n",
    "\n",
    "The cosine similarity measure takes on values between -1 and 1. a value of 1 implies a completely similar status, while a value of 0 implies independence. We can also capture the negative similarity, that is a value of -1 implies that not only are the vectors not similar, but they are also completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosineSimilarity: (vec1: org.jblas.DoubleMatrix, vec2: org.jblas.DoubleMatrix)Double"
     ]
    }
   ],
   "source": [
    "def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double =\n",
    "{\n",
    "    vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try out our function on item 567. We will collect an item factor form out model using the lookup() method. We use the head() function to obtain the first value. We create the DoubleMatrix object from the created Array[Double]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res40: Double = 0.9999999999999998"
     ]
    }
   ],
   "source": [
    "val itemId = 567\n",
    "val itemFactor = model.productFeatures.lookup(itemId).head\n",
    "val itemVector = new DoubleMatrix(itemFactor)\n",
    "cosineSimilarity(itemVector, itemVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similarity metric should measure how close two vectors are to each other. Above, we can see that our metric shows that the item vector is idetical to itself. Below, we apply our similarity metric to each item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sims: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[469] at map at <console>:54"
     ]
    }
   ],
   "source": [
    "val sims = model.productFeatures.map{ case (id, factor) =>\n",
    "                                    val factorVector = new DoubleMatrix(factor)\n",
    "                                    val sim = cosineSimilarity(factorVector, itemVector)\n",
    "                                    (id, sim)\n",
    "                                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compute the top 10 most similar items by sorting out the similarity score for each item. We use the top() function, which computes the top-K results in a distributed fashion instead of using collect9) to return all the data to the driver and sorting it locally. Here, we tell Spark how to sort the paris in the sims RDD. To do this, we pass the Ordering object, which tells Spark that it should sort by the value in the key-value pair (a.k.a. sort by similarity). We then print the 10 items with the highest computed similarity metric to our given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567,0.9999999999999998)\n",
      "(1222,0.7208308624354276)\n",
      "(184,0.7015866096597198)\n",
      "(1002,0.7000642613423828)\n",
      "(271,0.6862652733768038)\n",
      "(550,0.6824989059373686)\n",
      "(152,0.6810681685408917)\n",
      "(219,0.6772090230791078)\n",
      "(448,0.6755880103107036)\n",
      "(563,0.6738665950586353)"
     ]
    }
   ],
   "source": [
    "val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
    "println(sortedSims.take(10).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Inspecting similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we observe the title of our chosen movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wes Craven's New Nightmare (1994)"
     ]
    }
   ],
   "source": [
    "println(titles(itemId))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we look at the titles of the most similar movies. We take the top 11 so that we can exclude our given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res43: String = \n",
      "(Judgment Night (1993),0.7208308624354276)\n",
      "(Army of Darkness (1993),0.7015866096597198)\n",
      "(Pest, The (1997),0.7000642613423828)\n",
      "(Starship Troopers (1997),0.6862652733768038)\n",
      "(Die Hard: With a Vengeance (1995),0.6824989059373686)\n",
      "(Sleeper (1973),0.6810681685408917)\n",
      "(Nightmare on Elm Street, A (1984),0.6772090230791078)\n",
      "(Omen, The (1976),0.6755880103107036)\n",
      "(Stephen King's The Langoliers (1995),0.6738665950586353)\n",
      "(Men in Black (1997),0.6730776169626559)"
     ]
    }
   ],
   "source": [
    "val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
    "sortedSims2.slice(1,11).map{ case (id, sim) => (titles(id), sim)}.mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect whether the model we have trained is a good model or not, we need to be able to evaluate its predictive performance in some way. Evalution metrics are measures of a model's predictive capability or accuracy. Some are direct measures of how well a model predicts the model's target variable while others are concerned with how well the model performs at predicting things that mights not be directly optimized in the model but are often closer to what we care about in the real world. \n",
    "\n",
    "These metrics provide a standardized way of comparing the performance of the same model with different parameter setting and of comparing performance across different models. Using these metrics, we can perform model selection to choose the best-performing model from the set of models we wish to evalute.\n",
    "\n",
    "Below, we will calculate two common evaluation metrics used in reommender systems and collaborative filtering models: Mean Squared Error and Mean average precision at K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE) is a direct measure of the reconstruction error of the user-item rating matrix. It is also the objective function being minimized in certain models, specifically many matrix-factorization techniques. Therefore, it is commonly used in explicit ratings settings. \n",
    "\n",
    "It is defined as the sum of the squared errors divided by the number of observations. The squared error, in turn, is the square of the difference between the predicted rating for a given user-item pair and the actual rating. \n",
    "\n",
    "Here, we take the first rating for user 789 from the moviesForUser set of Ratings previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(789,1012,4.0)"
     ]
    }
   ],
   "source": [
    "val actualRating = moviesForUser.take(1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the model's predicted rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictedRating: Double = 4.074667240270278"
     ]
    }
   ],
   "source": [
    "val predictedRating = model.predict(789, actualRating.product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the squred error between the actual rating and the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squaredError: Double = 0.005575196769579448"
     ]
    }
   ],
   "source": [
    "val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the overall MSE for the dataset, we need to compute the squared error for each entry, sum them up, and divide them by the number of ratings.\n",
    "\n",
    "Firstly, we extract the user and product IDs from the ratings RDD and make predictions for each user-item pair using model.predict(). We will then use the user-item pair as the key and the predicted rating as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[481] at map at <console>:48"
     ]
    }
   ],
   "source": [
    "val usersProducts = ratings.map{ case Rating(user, product, rating) => (user, product)}\n",
    "val predictions = model.predict(usersProducts).map{\n",
    "    case Rating(user, product, rating) => ((user, product), rating)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the actual ratings and also map the ratings RDD so that the user-item pair is the key and the actual rating is the value. We can now join the two RDDs together to create a new RDD with the actual and predicted ratings for each user-item combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratingsAndPredictions: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[485] at join at <console>:52"
     ]
    }
   ],
   "source": [
    "val ratingsAndPredictions = ratings.map{\n",
    "    case Rating(user, product, rating) => ((user, product), rating)\n",
    "}.join(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the MSE by summing up the squared errors using reduce and dividing by the count method of the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Suared Error = 0.08470571763977897"
     ]
    }
   ],
   "source": [
    "val MSE = ratingsAndPredictions.map{\n",
    "    case ((user, product), (actual, predicted)) => math.pow((actual - predicted), 2)\n",
    "}.reduce(_ + _) / ratingsAndPredictions.count\n",
    "println(\"Mean Suared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to use the Root Mean Squared Error (RMSE), which is the square root of the MSE metric. This is somewhat more interpretable, since it is the same units as the underlying data. It is equivalent to the standard deviation of the differences between the predicted and actual ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error = 0.2910424670727263"
     ]
    }
   ],
   "source": [
    "val RMSE = math.sqrt(MSE)\n",
    "println(\"Root Mean Squared Error = \" + RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision at K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean average precision at K (MAPK) is the mean of the average precision at K (APK) metric across all instances in the dataset. APK is a metric commonly used in information retrieval. APK is a measure of the average relevance scores of a set of the top-K documents presented in response to a query. For every query instance, we compare the set of top-K results with the set of actual relevant documents.\n",
    "\n",
    "In the APK metric, the order of the result set matters in that the APK score would be higher if the result documents are both relevant nad the relevant documents are presented higher in the results. Therefore, it is a solid metric for recommender systems in that typically we would compute the top-K recommended items for each user and present these to the user. APK and other ranking-based metrics are also more appropriate evaluation measures for implicit datasets. Here, MSE makes less sense. \n",
    "\n",
    "In order to evaluate our model, we use APK, where each user is the equivalent of a query, and the set of top-K recommended items is the document result set. The relevant documents in our case, is the set of items that a user interacted with. Therefore, APK attempts to measure how good our model is at predicting items that a user will find relevant and choose to interact with. \n",
    "\n",
    "Here is our fucntion to compute the APK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgPrecisionK: (actual: Seq[Int], predicted: Seq[Int], k: Int)Double"
     ]
    }
   ],
   "source": [
    "def avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int):\n",
    "Double = {\n",
    "    val predK = predicted.take(k)\n",
    "    var score = 0.0\n",
    "    var numHits = 0.0\n",
    "    for ((p, i) <- predK.zipWithIndex) {\n",
    "        if (actual.contains(p)) {\n",
    "            numHits += 1.0\n",
    "            score += numHits / (i.toDouble + 1.0)\n",
    "        }\n",
    "    }\n",
    "    if (actual.isEmpty) {\n",
    "        1.0\n",
    "    } else {\n",
    "        score / scala.math.min(actual.size, k).toDouble\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the APK metric for example user 789. \n",
    "\n",
    "First we extract the actual movie IDs for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actualMovies: Seq[Int] = ArrayBuffer(1012, 127, 475, 93, 1161, 286, 293, 9, 50, 294, 181, 1, 1008, 508, 284, 1017, 137, 111, 742, 248, 249, 1007, 591, 150, 276, 151, 129, 100, 741, 288, 762, 628, 124)"
     ]
    }
   ],
   "source": [
    "val actualMovies = moviesForUser.map(_.product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the movie recommendations we made previously to compute the APK score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictedMovies: Array[Int] = Array(182, 317, 156, 192, 56, 87, 58, 641, 530, 157)"
     ]
    }
   ],
   "source": [
    "val predictedMovies = topKRecs.map(_.product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will produce the average precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apk10: Double = 0.0"
     ]
    }
   ],
   "source": [
    "val apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the APK for each user and average them to compute the overall MAPK, we need to generate the list of recommendations for each user in our dataset. While this can be fairly intensive on a large scale, we can distribute the computation using our Spark functionality. However, one limitation is that each worder must have the full item-factor matrix available so that it can compute the dot product between the relevant user matrix available so that it can compute the dot product between the relevant user vector and all item vectors. This can be a problem when the number of items is extremely high as the item matrix must fit in the memory of one machine. \n",
    "\n",
    "To do this, we first collect the item factors and form a DoubleMatrix object from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682,50)"
     ]
    }
   ],
   "source": [
    "val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\n",
    "val itemMatrix = new DoubleMatrix(itemFactors)\n",
    "println(itemMatrix.rows, itemMatrix.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we distribute the item matrix as a broadcast variable so that it is available on each worker node in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imBroadcast: org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = Broadcast(121)"
     ]
    }
   ],
   "source": [
    "val imBroadcast = sc.broadcast(itemMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the recommendations for each user. We do this by applying a map function to each user factor within which we will perform a matrix multiplication between the user-factor vector and the movie-factor matrix. The result is a vector with the predicted rating for each movie. We then sort these predictions by the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MapPartitionsRDD[488] at map at <console>:54"
     ]
    }
   ],
   "source": [
    "val allRecs = model.userFeatures.map{ case (userId, array) =>\n",
    "    val userVector = new DoubleMatrix(array)\n",
    "    val scores = imBroadcast.value.mmul(userVector)\n",
    "    val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n",
    "    val recommendedIds = sortedWithId.zipWithIndex.map(_._2 + 1).toSeq\n",
    "    (userId, recommendedIds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a list of movie IDs for each user to pass into our APK fucntion as the actual argument. We already have the ratings RDD ready, so we can extract the user nad movie IDs from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userMovies: org.apache.spark.rdd.RDD[(Int, Iterable[(Int, Int)])] = ShuffledRDD[491] at groupBy at <console>:44"
     ]
    }
   ],
   "source": [
    "val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use Sparks join() operator to join these two RDDs together on the user ID key. Then, for every user, we have the list of actual and predicted movie IDs that we can pass to our APK function. In a monner similar to how we computed MSE, we will sum each of these APK scores using a reduce action and divide by the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision at K = 0.1300756619367436"
     ]
    }
   ],
   "source": [
    "val K = 10\n",
    "val MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) =>\n",
    "    val actual = actualWithIds.map(_._2).toSeq\n",
    "    avgPrecisionK(actual, predicted, K)\n",
    "}.reduce(_ + _) / allRecs.count\n",
    "println(\"Mean Average Precision at K = \" + MAPK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using MLlib, which provides convenience functions to compute MSE, RMSE, and MAPK in the RegressionMetrics and RankingMetrics classes.\n",
    "\n",
    "First, we compute the MSE and RMSE metrics using RegressionMetrics. We instantiate the predicted and true values for each data point. We again use the ratingsAndPredictions RDD we computed in the previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.08470571763977897\n",
      "Root Mean Squared Error = 0.2910424670727263"
     ]
    }
   ],
   "source": [
    "val predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (predicted, actual)) => (predicted, actual) }\n",
    "val regressionMetrics = new RegressionMetrics(predictedAndTrue)\n",
    "\n",
    "println(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError +\n",
    "        \"\\nRoot Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute ranking-based evaluation metrics using MLlib's RankingMetrics class. Similarly, to our own average precision function, we need to pass in an RDD of key-value pairs, where the key is an Array of predicted item IDs for a user, while the value is an array of actual item IDs.\n",
    "\n",
    "First, we calculate MAP using RankingMetrics()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision = 0.15884678068634858"
     ]
    }
   ],
   "source": [
    "val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) =>\n",
    "    val actual = actualWithIds.map(_._2)\n",
    "    (predicted.toArray, actual.toArray)\n",
    "}\n",
    "val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\n",
    "println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use our function to compute the MAP in exactly the same way as we did previously, except that we set K to a very high value (i.e. 2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision = 0.15884678068634836"
     ]
    }
   ],
   "source": [
    "val MAPK2000 = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
    "    val actual = actualWithIds.map(_._2).toSeq\n",
    "    avgPrecisionK(actual, predicted, 2000)\n",
    "}.reduce(_ + _) / allRecs.count\n",
    "println(\"Mean Average Precision = \" + MAPK2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this [mean average precision](http://fastml.com/what-you-wanted-to-know-about-mean-average-precision/)? Think about it in the context of how diverse movies and ratings can be, and how often movie recommendations work out for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
